# Model configuration
model:
  name: "ddpm"
  backbone: "unet"
  image_size: 32
  channels: 3
  time_embedding_dim: 256
  base_channels: 128
  channel_multipliers: [1, 2, 2, 2]
  attention_resolutions: [16]
  num_res_blocks: 2
  dropout: 0.1

# Diffusion process configuration
diffusion:
  timesteps: 1000
  beta_schedule: "linear"  # linear, cosine
  beta_start: 0.0001
  beta_end: 0.02

# Training configuration
training:
  framework: "pytorch"  # pytorch or paddle
  batch_size: 128
  num_epochs: 500
  learning_rate: 2e-4
  weight_decay: 0.0
  ema_decay: 0.9999
  gradient_clip: 1.0
  warmup_steps: 5000

# Dataset configuration
data:
  name: "cifar10"
  root: "./data"
  num_workers: 4
  pin_memory: true

# Evaluation configuration
evaluation:
  eval_every: 5000  # Steps between evaluations
  save_every: 10000  # Steps between checkpoints
  sample_size: 64  # Number of samples to generate
  fid_samples: 50000  # Number of samples for FID
  lpips_samples: 1000  # Number of samples for LPIPS

# Logging configuration
logging:
  project: "cifar10_diffusion"
  log_dir: "runs/cifar10"
  checkpoint_dir: "checkpoints/cifar10"
  sample_dir: "samples/cifar10"
  log_every: 100  # Steps between logging

# Distributed training configuration
distributed:
  enabled: false
  backend: "nccl"
  world_size: 1
  rank: 0 